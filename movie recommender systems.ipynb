{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Embedding, Flatten, Input, dot\n",
    "from keras import Model\n",
    "from IPython.display import SVG\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define attributes of u.item\n",
    "FieldsMovies = ['movieID', 'movieTitle', 'releaseDate', 'videoReleaseDate', 'IMDbURL', 'unknown', 'action', 'adventure',\n",
    "      'animation', 'childrens', 'comedy', 'crime', 'documentary', 'drama', 'fantasy', 'filmNoir', 'horror',\n",
    "      'musical', 'mystery', 'romance','sciFi', 'thriller', 'war', 'western']\n",
    "\n",
    "# Read the data into dataframes\n",
    "DataDf = pd.read_csv(\"ml-100k/u.data\", sep='\\t', names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "Itemdata = pd.read_csv(\"ml-100k/u.item\", sep='|', encoding = \"ISO-8859-1\", names=FieldsMovies)\n",
    "\n",
    "# Setups\n",
    "FindID = Itemdata.movieTitle.to_dict()  # key: movieID-1, value:movie title\n",
    "NumItems = len(DataDf.item_id.unique())\n",
    "NumUsers = len(DataDf.user_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0      196      242       3\n",
       "1      186      302       3\n",
       "2       22      377       1\n",
       "3      244       51       2\n",
       "4      166      346       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataDf = DataDf.drop('timestamp', axis=1)\n",
    "DataDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>462.48475</td>\n",
       "      <td>425.530130</td>\n",
       "      <td>3.529860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>266.61442</td>\n",
       "      <td>330.798356</td>\n",
       "      <td>1.125674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>254.00000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.00000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>682.00000</td>\n",
       "      <td>631.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>943.00000</td>\n",
       "      <td>1682.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id        item_id         rating\n",
       "count  100000.00000  100000.000000  100000.000000\n",
       "mean      462.48475     425.530130       3.529860\n",
       "std       266.61442     330.798356       1.125674\n",
       "min         1.00000       1.000000       1.000000\n",
       "25%       254.00000     175.000000       3.000000\n",
       "50%       447.00000     322.000000       4.000000\n",
       "75%       682.00000     631.000000       4.000000\n",
       "max       943.00000    1682.000000       5.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataDf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many unique users and items (movies) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 943\n",
      "Number of Movies: 1682\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Users: {}\\nNumber of Movies: {}\".format(NumUsers, NumItems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(DataDf, test_size=0.2)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildItemUserMatrix(dataDf):\n",
    "    '''\n",
    "    Input\n",
    "    - datadf: A pandas dataframe of movielens dataset\n",
    "\n",
    "    Ouput\n",
    "    - matrix: A 2d numpy array of Item-User Matrix (numItems x numUsers)\n",
    "    '''\n",
    "    dataMatrix = np.zeros((NumItems, NumUsers), dtype=np.int8)\n",
    "\n",
    "    # Populate the matrix based on the dataset\n",
    "    for (index, userID, itemID, rating) in dataDf.itertuples():\n",
    "        dataMatrix[itemID-1, userID-1] = rating\n",
    "    return dataMatrix\n",
    "\n",
    "def getSimilarMovies(similarity, movieID, k=5):\n",
    "    '''\n",
    "    Recommend k-number of similar movie by cosine similarity metric\n",
    "    '''\n",
    "    movieVec = similarity[movieID-1]\n",
    "    movieVec = np.delete(movieVec, movieID-1)\n",
    "    sortedIdx = np.argsort(movieVec)[::-1]\n",
    "    print(\"Did you like '{}'?\\n\".format(FindID[movieID-1]))\n",
    "    print(\"What we recommend for you\")\n",
    "    for i in range(k):\n",
    "        movietitle = FindID[sortedIdx[i+1]+1]\n",
    "        print(\"{}: {}\".format(i+1, movietitle))\n",
    "\n",
    "def whichMovie(movieID):\n",
    "    return FindID[movieID-1]\n",
    "\n",
    "def predictRatings(trainDf, similarity):\n",
    "    '''\n",
    "    Predict empty ratings by using item-cosine-similarity\n",
    "    '''\n",
    "    dataMatrix = buildItemUserMatrix(trainDf)\n",
    "    predictionMatrix = np.zeros((NumItems, NumUsers))\n",
    "    \n",
    "    # loop over non-rated items\n",
    "    for u in range(NumUsers):\n",
    "        userVector = dataMatrix[:,u]\n",
    "        nonzeros = userVector.nonzero()\n",
    "        itemRatings = userVector[nonzeros]\n",
    "        zeros = np.argwhere(userVector == 0).flatten()\n",
    "\n",
    "        for i in zeros:\n",
    "            # Get the similarity score for each of the items that provided rating by this user\n",
    "            itemSims = similarity[i,:][nonzeros]\n",
    "\n",
    "            if itemSims.sum() == 0:\n",
    "                continue\n",
    "            else:\n",
    "                #Predict score based on item-item similarity\n",
    "                predictionMatrix[i,u] = (itemRatings * itemSims).sum() / itemSims.sum()\n",
    "    \n",
    "    return predictionMatrix\n",
    "\n",
    "def recommendMovies(userID, **options):\n",
    "    '''\n",
    "    Recommend movies for the user with userID\n",
    "    '''\n",
    "    # Read options args\n",
    "    model = options.pop('model', None)\n",
    "    trainDf = options.pop('train_data', None)\n",
    "    similarity = options.pop('similarity', None)\n",
    "    predictionMatrix = options.pop('prediction_matrix', None)\n",
    "    k = options.pop('k', 10)\n",
    "    \n",
    "    if options:\n",
    "        raise TypeError(\"Invalid parameters passed: %s\" % str(options))\n",
    "\n",
    "    \n",
    "    \n",
    "    recMovies = []\n",
    "    \n",
    "    # if trained model was passed in\n",
    "    if model:\n",
    "        pairs = [ np.array([userID] * NumUsers),\n",
    "                 np.array([itemID for itemID in range(1, NumItems+1)]) ]\n",
    "        pred = model.predict(pairs)\n",
    "        argmaxIdx = np.argsort(pred, axis=0)[::-1].squeeze()\n",
    "        argmaxIdx = argmaxIdx[:k]\n",
    "\n",
    "        # Get the k-highest scored movies in the predictions list\n",
    "        for i in argmaxIdx:\n",
    "            recMovies.append(FindID[i])\n",
    "\n",
    "        return recMovies\n",
    "    \n",
    "    # if similarity matrix was passed in, use cosine similarity\n",
    "    elif similarity is not None:\n",
    "\n",
    "        # if trainDf is None, raise error\n",
    "        if trainDf is None:\n",
    "            raise BaseException(\"Invalid data types passed\")\n",
    "        \n",
    "        # if prediction matrix is None, calculate it\n",
    "        if predictionMatrix is None:\n",
    "            predictionMatrix = predictRatings(trainDf, similarity)\n",
    "         \n",
    "        userVector = predictionMatrix[:, userID-1]\n",
    "        argmaxIdx = np.argsort(userVector, axis=0)[::-1].squeeze()\n",
    "        \n",
    "        if len(argmaxIdx) >= k:\n",
    "            argmaxIdx = argmaxIdx[:k]\n",
    "        \n",
    "        # Get the k-highest scored movies in the predictions list\n",
    "        for i in argmaxIdx:\n",
    "            recMovies.append(FindID[i])\n",
    "\n",
    "        return recMovies\n",
    "\n",
    "def usersBestMovies(dataDf, userID):\n",
    "    '''\n",
    "    Find the best rated movies for the user\n",
    "    '''\n",
    "    # if the userID is not found in the data, print error message\n",
    "    if len(dataDf.loc[dataDf['user_id'] == userID]) == 0:\n",
    "        print(\"'User_ID {}' is not in our database!\".format(userID))\n",
    "        return\n",
    "    \n",
    "    # Return all the movies which were rated the highest score by the user\n",
    "    maxrating = dataDf.loc[dataDf['user_id']==userID].rating.max()\n",
    "    bestMovieIDs = dataDf.loc[(dataDf['user_id']==userID) &\n",
    "                            (dataDf['rating']==maxrating)].item_id\n",
    "    \n",
    "    # Store the best movies\n",
    "    bestMovies = []\n",
    "    for movieID in bestMovieIDs:\n",
    "        bestMovies.append(FindID[movieID-1])\n",
    "        \n",
    "    return bestMovies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataMatrix = buildItemUserMatrix(DataDf)\n",
    "similarity = cosine_similarity(dataMatrix)\n",
    "trainMatrix = buildItemUserMatrix(train)\n",
    "testMatrix = buildItemUserMatrix(test)\n",
    "pred_Matrix = predictRatings(train, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error 1.014304047219637\n",
      "Root Mean Square Error (rounded) 1.0517128885774862\n"
     ]
    }
   ],
   "source": [
    "# add predicted scores in the test dataframe\n",
    "for ind, userID, itemID, rating in test.itertuples():\n",
    "    test.loc[ind, 'pred_cosSim'] = pred_Matrix[itemID-1, userID-1]\n",
    "\n",
    "rmse_similarity = np.sqrt(mean_squared_error(test.rating, test.pred_cosSim))\n",
    "rmse_similarity_round = np.sqrt(mean_squared_error(test.rating, np.round(test.pred_cosSim)))\n",
    "print(\"Root Mean Square Error {}\".format(rmse_similarity))\n",
    "print(\"Root Mean Square Error (rounded) {}\".format(rmse_similarity_round))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_latent_factors = 3\n",
    "\n",
    "# Define movies (items) matrix\n",
    "# shape: (batch_size, embedding_size)\n",
    "movie_input = Input(shape=[1], name='Item')\n",
    "movie_embedding = Embedding(input_dim=NumItems+1, output_dim=n_latent_factors,\n",
    "                                         name='Movie-Embedding')(movie_input)\n",
    "movie_vec = Flatten(name='FlattenMovies')(movie_embedding)\n",
    "\n",
    "# Define users matrix\n",
    "user_input = Input(shape=[1], name='User')\n",
    "user_embedding = Embedding(input_dim=NumUsers+1, output_dim=n_latent_factors,\n",
    "                                        name='User-Embedding')(user_input)\n",
    "user_vec = Flatten(name='FlattenUsers')(user_embedding)\n",
    "\n",
    "# Build model\n",
    "prod = dot([movie_vec, user_vec], axes=1, name='DotProduct')\n",
    "\n",
    "model = Model([user_input, movie_input], prod)\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Item (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "User (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Movie-Embedding (Embedding)     (None, 1, 3)         5049        Item[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "User-Embedding (Embedding)      (None, 1, 3)         2832        User[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "FlattenMovies (Flatten)         (None, 3)            0           Movie-Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "FlattenUsers (Flatten)          (None, 3)            0           User-Embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "DotProduct (Dot)                (None, 1)            0           FlattenMovies[0][0]              \n",
      "                                                                 FlattenUsers[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,881\n",
      "Trainable params: 7,881\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 11.90445, saving model to weights.hdf5\n",
      " - 3s - loss: 13.4098 - val_loss: 11.9045\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: val_loss improved from 11.90445 to 5.36067, saving model to weights.hdf5\n",
      " - 2s - loss: 8.4757 - val_loss: 5.3607\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.36067 to 2.58545, saving model to weights.hdf5\n",
      " - 2s - loss: 3.6576 - val_loss: 2.5855\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.58545 to 1.76567, saving model to weights.hdf5\n",
      " - 3s - loss: 2.0643 - val_loss: 1.7657\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.76567 to 1.40348, saving model to weights.hdf5\n",
      " - 2s - loss: 1.5061 - val_loss: 1.4035\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.40348 to 1.21290, saving model to weights.hdf5\n",
      " - 3s - loss: 1.2380 - val_loss: 1.2129\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.21290 to 1.10380, saving model to weights.hdf5\n",
      " - 2s - loss: 1.0932 - val_loss: 1.1038\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.10380 to 1.04000, saving model to weights.hdf5\n",
      " - 2s - loss: 1.0095 - val_loss: 1.0400\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.04000 to 0.99915, saving model to weights.hdf5\n",
      " - 2s - loss: 0.9593 - val_loss: 0.9991\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.99915 to 0.97316, saving model to weights.hdf5\n",
      " - 2s - loss: 0.9281 - val_loss: 0.9732\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.97316 to 0.95646, saving model to weights.hdf5\n",
      " - 2s - loss: 0.9076 - val_loss: 0.9565\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.95646 to 0.94585, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8944 - val_loss: 0.9458\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.94585 to 0.93668, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8846 - val_loss: 0.9367\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.93668 to 0.93275, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8778 - val_loss: 0.9328\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.93275 to 0.92910, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8726 - val_loss: 0.9291\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.92910 to 0.92644, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8687 - val_loss: 0.9264\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.92644 to 0.92485, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8653 - val_loss: 0.9248\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.92485 to 0.92270, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8628 - val_loss: 0.9227\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.92270 to 0.92086, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8605 - val_loss: 0.9209\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.92086 to 0.91878, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8589 - val_loss: 0.9188\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.91878 to 0.91822, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8572 - val_loss: 0.9182\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 2s - loss: 0.8557 - val_loss: 0.9192\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.91822 to 0.91751, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8546 - val_loss: 0.9175\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 2s - loss: 0.8533 - val_loss: 0.9183\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.91751 to 0.91704, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8522 - val_loss: 0.9170\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 2s - loss: 0.8513 - val_loss: 0.9180\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.91704 to 0.91566, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8506 - val_loss: 0.9157\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 2s - loss: 0.8495 - val_loss: 0.9160\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 2s - loss: 0.8485 - val_loss: 0.9157\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.91566 to 0.91408, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8474 - val_loss: 0.9141\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      " - 2s - loss: 0.8465 - val_loss: 0.9153\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 2s - loss: 0.8455 - val_loss: 0.9153\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.91408 to 0.91316, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8441 - val_loss: 0.9132\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.91316 to 0.91305, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8427 - val_loss: 0.9130\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.91305 to 0.91142, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8414 - val_loss: 0.9114\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.91142 to 0.91031, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8396 - val_loss: 0.9103\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.91031 to 0.90923, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8379 - val_loss: 0.9092\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      " - 2s - loss: 0.8361 - val_loss: 0.9093\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.90923 to 0.90704, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8338 - val_loss: 0.9070\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.90704 to 0.90619, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8314 - val_loss: 0.9062\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.90619 to 0.90399, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8288 - val_loss: 0.9040\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.90399 to 0.90322, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8257 - val_loss: 0.9032\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.90322 to 0.90164, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8228 - val_loss: 0.9016\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.90164 to 0.89949, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8194 - val_loss: 0.8995\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.89949 to 0.89767, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8158 - val_loss: 0.8977\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.89767 to 0.89532, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8117 - val_loss: 0.8953\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.89532 to 0.89400, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8081 - val_loss: 0.8940\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.89400 to 0.89133, saving model to weights.hdf5\n",
      " - 2s - loss: 0.8040 - val_loss: 0.8913\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.89133 to 0.88881, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7996 - val_loss: 0.8888\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.88881 to 0.88753, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7955 - val_loss: 0.8875\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.88753 to 0.88597, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7915 - val_loss: 0.8860\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.88597 to 0.88419, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7873 - val_loss: 0.8842\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.88419 to 0.88218, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7831 - val_loss: 0.8822\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.88218 to 0.88070, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7791 - val_loss: 0.8807\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.88070 to 0.87825, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7753 - val_loss: 0.8782\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.87825 to 0.87750, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7716 - val_loss: 0.8775\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      " - 2s - loss: 0.7680 - val_loss: 0.8779\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.87750 to 0.87672, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7645 - val_loss: 0.8767\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.87672 to 0.87651, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7613 - val_loss: 0.8765\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.87651 to 0.87614, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7581 - val_loss: 0.8761\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.87614 to 0.87524, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7552 - val_loss: 0.8752\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      " - 2s - loss: 0.7524 - val_loss: 0.8755\n",
      "Epoch 63/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00063: val_loss improved from 0.87524 to 0.87442, saving model to weights.hdf5\n",
      " - 2s - loss: 0.7496 - val_loss: 0.8744\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      " - 2s - loss: 0.7471 - val_loss: 0.8750\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      " - 2s - loss: 0.7447 - val_loss: 0.8755\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      " - 2s - loss: 0.7424 - val_loss: 0.8754\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      " - 2s - loss: 0.7403 - val_loss: 0.8754\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      " - 2s - loss: 0.7382 - val_loss: 0.8756\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      " - 2s - loss: 0.7361 - val_loss: 0.8755\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      " - 2s - loss: 0.7345 - val_loss: 0.8759\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      " - 2s - loss: 0.7330 - val_loss: 0.8760\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      " - 3s - loss: 0.7311 - val_loss: 0.8766\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      " - 2s - loss: 0.7297 - val_loss: 0.8767\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      " - 2s - loss: 0.7283 - val_loss: 0.8780\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      " - 2s - loss: 0.7270 - val_loss: 0.8778\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      " - 2s - loss: 0.7257 - val_loss: 0.8783\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      " - 2s - loss: 0.7244 - val_loss: 0.8785\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      " - 2s - loss: 0.7233 - val_loss: 0.8778\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      " - 2s - loss: 0.7222 - val_loss: 0.8777\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      " - 2s - loss: 0.7213 - val_loss: 0.8785\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      " - 2s - loss: 0.7205 - val_loss: 0.8791\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      " - 2s - loss: 0.7194 - val_loss: 0.8796\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      " - 2s - loss: 0.7185 - val_loss: 0.8795\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      " - 2s - loss: 0.7176 - val_loss: 0.8795\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      " - 2s - loss: 0.7168 - val_loss: 0.8802\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      " - 2s - loss: 0.7161 - val_loss: 0.8813\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      " - 2s - loss: 0.7154 - val_loss: 0.8801\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      " - 2s - loss: 0.7147 - val_loss: 0.8797\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      " - 2s - loss: 0.7140 - val_loss: 0.8814\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      " - 2s - loss: 0.7134 - val_loss: 0.8811\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      " - 2s - loss: 0.7128 - val_loss: 0.8818\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      " - 2s - loss: 0.7122 - val_loss: 0.8807\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      " - 2s - loss: 0.7116 - val_loss: 0.8817\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      " - 2s - loss: 0.7113 - val_loss: 0.8823\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      " - 2s - loss: 0.7104 - val_loss: 0.8817\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      " - 2s - loss: 0.7099 - val_loss: 0.8822\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      " - 2s - loss: 0.7095 - val_loss: 0.8833\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      " - 2s - loss: 0.7091 - val_loss: 0.8828\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      " - 2s - loss: 0.7086 - val_loss: 0.8847\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      " - 2s - loss: 0.7082 - val_loss: 0.8850\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      " - 2s - loss: 0.7078 - val_loss: 0.8844\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: val_loss did not improve\n",
      " - 2s - loss: 0.7073 - val_loss: 0.8841\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: val_loss did not improve\n",
      " - 2s - loss: 0.7069 - val_loss: 0.8864\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      " - 2s - loss: 0.7063 - val_loss: 0.8854\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      " - 2s - loss: 0.7061 - val_loss: 0.8863\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      " - 2s - loss: 0.7058 - val_loss: 0.8851\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: val_loss did not improve\n",
      " - 2s - loss: 0.7053 - val_loss: 0.8861\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: val_loss did not improve\n",
      " - 2s - loss: 0.7051 - val_loss: 0.8845\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: val_loss did not improve\n",
      " - 2s - loss: 0.7046 - val_loss: 0.8846\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: val_loss did not improve\n",
      " - 2s - loss: 0.7041 - val_loss: 0.8865\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: val_loss did not improve\n",
      " - 2s - loss: 0.7041 - val_loss: 0.8874\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: val_loss did not improve\n",
      " - 2s - loss: 0.7038 - val_loss: 0.8879\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: val_loss did not improve\n",
      " - 2s - loss: 0.7033 - val_loss: 0.8870\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: val_loss did not improve\n",
      " - 2s - loss: 0.7032 - val_loss: 0.8866\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: val_loss did not improve\n",
      " - 2s - loss: 0.7028 - val_loss: 0.8871\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: val_loss did not improve\n",
      " - 2s - loss: 0.7026 - val_loss: 0.8867\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      " - 2s - loss: 0.7023 - val_loss: 0.8880\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: val_loss did not improve\n",
      " - 2s - loss: 0.7021 - val_loss: 0.8877\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: val_loss did not improve\n",
      " - 2s - loss: 0.7017 - val_loss: 0.8876\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      " - 2s - loss: 0.7016 - val_loss: 0.8885\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: val_loss did not improve\n",
      " - 2s - loss: 0.7014 - val_loss: 0.8887\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: val_loss did not improve\n",
      " - 2s - loss: 0.7011 - val_loss: 0.8892\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: val_loss did not improve\n",
      " - 2s - loss: 0.7008 - val_loss: 0.8879\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: val_loss did not improve\n",
      " - 2s - loss: 0.7007 - val_loss: 0.8890\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: val_loss did not improve\n",
      " - 2s - loss: 0.7003 - val_loss: 0.8903\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      " - 2s - loss: 0.7001 - val_loss: 0.8904\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      " - 2s - loss: 0.7000 - val_loss: 0.8912\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: val_loss did not improve\n",
      " - 2s - loss: 0.6998 - val_loss: 0.8918\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      " - 2s - loss: 0.6997 - val_loss: 0.8910\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      " - 2s - loss: 0.6994 - val_loss: 0.8914\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: val_loss did not improve\n",
      " - 2s - loss: 0.6993 - val_loss: 0.8929\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      " - 2s - loss: 0.6991 - val_loss: 0.8922\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: val_loss did not improve\n",
      " - 2s - loss: 0.6989 - val_loss: 0.8925\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: val_loss did not improve\n",
      " - 2s - loss: 0.6985 - val_loss: 0.8920\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: val_loss did not improve\n",
      " - 2s - loss: 0.6984 - val_loss: 0.8928\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: val_loss did not improve\n",
      " - 2s - loss: 0.6983 - val_loss: 0.8929\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: val_loss did not improve\n",
      " - 2s - loss: 0.6983 - val_loss: 0.8932\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: val_loss did not improve\n",
      " - 2s - loss: 0.6979 - val_loss: 0.8946\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: val_loss did not improve\n",
      " - 2s - loss: 0.6978 - val_loss: 0.8929\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: val_loss did not improve\n",
      " - 2s - loss: 0.6977 - val_loss: 0.8938\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: val_loss did not improve\n",
      " - 2s - loss: 0.6975 - val_loss: 0.8934\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: val_loss did not improve\n",
      " - 2s - loss: 0.6973 - val_loss: 0.8942\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: val_loss did not improve\n",
      " - 2s - loss: 0.6972 - val_loss: 0.8945\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: val_loss did not improve\n",
      " - 2s - loss: 0.6971 - val_loss: 0.8943\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: val_loss did not improve\n",
      " - 2s - loss: 0.6970 - val_loss: 0.8951\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: val_loss did not improve\n",
      " - 2s - loss: 0.6968 - val_loss: 0.8961\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: val_loss did not improve\n",
      " - 2s - loss: 0.6966 - val_loss: 0.8968\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: val_loss did not improve\n",
      " - 2s - loss: 0.6965 - val_loss: 0.8974\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: val_loss did not improve\n",
      " - 2s - loss: 0.6964 - val_loss: 0.8963\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: val_loss did not improve\n",
      " - 2s - loss: 0.6962 - val_loss: 0.8962\n",
      "Epoch 151/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00151: val_loss did not improve\n",
      " - 2s - loss: 0.6962 - val_loss: 0.8978\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: val_loss did not improve\n",
      " - 2s - loss: 0.6961 - val_loss: 0.8977\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: val_loss did not improve\n",
      " - 2s - loss: 0.6959 - val_loss: 0.8965\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: val_loss did not improve\n",
      " - 2s - loss: 0.6959 - val_loss: 0.8972\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: val_loss did not improve\n",
      " - 2s - loss: 0.6958 - val_loss: 0.8998\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: val_loss did not improve\n",
      " - 2s - loss: 0.6956 - val_loss: 0.8998\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: val_loss did not improve\n",
      " - 2s - loss: 0.6954 - val_loss: 0.9002\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: val_loss did not improve\n",
      " - 2s - loss: 0.6954 - val_loss: 0.8997\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: val_loss did not improve\n",
      " - 2s - loss: 0.6956 - val_loss: 0.9012\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: val_loss did not improve\n",
      " - 2s - loss: 0.6951 - val_loss: 0.9013\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: val_loss did not improve\n",
      " - 2s - loss: 0.6950 - val_loss: 0.9016\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: val_loss did not improve\n",
      " - 2s - loss: 0.6951 - val_loss: 0.9018\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: val_loss did not improve\n",
      " - 2s - loss: 0.6948 - val_loss: 0.9018\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: val_loss did not improve\n",
      " - 2s - loss: 0.6948 - val_loss: 0.9023\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: val_loss did not improve\n",
      " - 2s - loss: 0.6946 - val_loss: 0.9012\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: val_loss did not improve\n",
      " - 2s - loss: 0.6945 - val_loss: 0.9012\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: val_loss did not improve\n",
      " - 2s - loss: 0.6946 - val_loss: 0.9024\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: val_loss did not improve\n",
      " - 2s - loss: 0.6946 - val_loss: 0.9031\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: val_loss did not improve\n",
      " - 2s - loss: 0.6942 - val_loss: 0.9030\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: val_loss did not improve\n",
      " - 2s - loss: 0.6942 - val_loss: 0.9031\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: val_loss did not improve\n",
      " - 2s - loss: 0.6940 - val_loss: 0.9042\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: val_loss did not improve\n",
      " - 2s - loss: 0.6940 - val_loss: 0.9041\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: val_loss did not improve\n",
      " - 2s - loss: 0.6940 - val_loss: 0.9039\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: val_loss did not improve\n",
      " - 2s - loss: 0.6938 - val_loss: 0.9047\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: val_loss did not improve\n",
      " - 2s - loss: 0.6938 - val_loss: 0.9051\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: val_loss did not improve\n",
      " - 2s - loss: 0.6936 - val_loss: 0.9051\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: val_loss did not improve\n",
      " - 2s - loss: 0.6936 - val_loss: 0.9059\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: val_loss did not improve\n",
      " - 2s - loss: 0.6937 - val_loss: 0.9062\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: val_loss did not improve\n",
      " - 3s - loss: 0.6933 - val_loss: 0.9052\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: val_loss did not improve\n",
      " - 3s - loss: 0.6934 - val_loss: 0.9060\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: val_loss did not improve\n",
      " - 2s - loss: 0.6932 - val_loss: 0.9065\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: val_loss did not improve\n",
      " - 2s - loss: 0.6933 - val_loss: 0.9069\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: val_loss did not improve\n",
      " - 2s - loss: 0.6932 - val_loss: 0.9063\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: val_loss did not improve\n",
      " - 2s - loss: 0.6930 - val_loss: 0.9065\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: val_loss did not improve\n",
      " - 2s - loss: 0.6928 - val_loss: 0.9068\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: val_loss did not improve\n",
      " - 2s - loss: 0.6930 - val_loss: 0.9079\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: val_loss did not improve\n",
      " - 2s - loss: 0.6929 - val_loss: 0.9083\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: val_loss did not improve\n",
      " - 3s - loss: 0.6927 - val_loss: 0.9092\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: val_loss did not improve\n",
      " - 3s - loss: 0.6927 - val_loss: 0.9091\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: val_loss did not improve\n",
      " - 2s - loss: 0.6926 - val_loss: 0.9093\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: val_loss did not improve\n",
      " - 2s - loss: 0.6925 - val_loss: 0.9094\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: val_loss did not improve\n",
      " - 2s - loss: 0.6926 - val_loss: 0.9088\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: val_loss did not improve\n",
      " - 2s - loss: 0.6927 - val_loss: 0.9095\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: val_loss did not improve\n",
      " - 2s - loss: 0.6923 - val_loss: 0.9097\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: val_loss did not improve\n",
      " - 2s - loss: 0.6923 - val_loss: 0.9102\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: val_loss did not improve\n",
      " - 2s - loss: 0.6923 - val_loss: 0.9095\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: val_loss did not improve\n",
      " - 2s - loss: 0.6922 - val_loss: 0.9100\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: val_loss did not improve\n",
      " - 2s - loss: 0.6922 - val_loss: 0.9109\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: val_loss did not improve\n",
      " - 2s - loss: 0.6920 - val_loss: 0.9117\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: val_loss did not improve\n",
      " - 2s - loss: 0.6920 - val_loss: 0.9119\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: val_loss did not improve\n",
      " - 2s - loss: 0.6920 - val_loss: 0.9111\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: val_loss did not improve\n",
      " - 2s - loss: 0.6919 - val_loss: 0.9120\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: val_loss did not improve\n",
      " - 2s - loss: 0.6918 - val_loss: 0.9127\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: val_loss did not improve\n",
      " - 2s - loss: 0.6918 - val_loss: 0.9130\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: val_loss did not improve\n",
      " - 2s - loss: 0.6916 - val_loss: 0.9139\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: val_loss did not improve\n",
      " - 2s - loss: 0.6917 - val_loss: 0.9137\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: val_loss did not improve\n",
      " - 2s - loss: 0.6916 - val_loss: 0.9136\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: val_loss did not improve\n",
      " - 2s - loss: 0.6916 - val_loss: 0.9131\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: val_loss did not improve\n",
      " - 3s - loss: 0.6914 - val_loss: 0.9141\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: val_loss did not improve\n",
      " - 2s - loss: 0.6914 - val_loss: 0.9138\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: val_loss did not improve\n",
      " - 2s - loss: 0.6915 - val_loss: 0.9146\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: val_loss did not improve\n",
      " - 2s - loss: 0.6913 - val_loss: 0.9148\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: val_loss did not improve\n",
      " - 2s - loss: 0.6913 - val_loss: 0.9153\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: val_loss did not improve\n",
      " - 2s - loss: 0.6913 - val_loss: 0.9147\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: val_loss did not improve\n",
      " - 2s - loss: 0.6912 - val_loss: 0.9154\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: val_loss did not improve\n",
      " - 2s - loss: 0.6911 - val_loss: 0.9163\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: val_loss did not improve\n",
      " - 2s - loss: 0.6912 - val_loss: 0.9161\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: val_loss did not improve\n",
      " - 2s - loss: 0.6911 - val_loss: 0.9166\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: val_loss did not improve\n",
      " - 2s - loss: 0.6910 - val_loss: 0.9166\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: val_loss did not improve\n",
      " - 2s - loss: 0.6910 - val_loss: 0.9162\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: val_loss did not improve\n",
      " - 2s - loss: 0.6910 - val_loss: 0.9166\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: val_loss did not improve\n",
      " - 2s - loss: 0.6908 - val_loss: 0.9172\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: val_loss did not improve\n",
      " - 3s - loss: 0.6909 - val_loss: 0.9171\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: val_loss did not improve\n",
      " - 2s - loss: 0.6908 - val_loss: 0.9183\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: val_loss did not improve\n",
      " - 2s - loss: 0.6908 - val_loss: 0.9181\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: val_loss did not improve\n",
      " - 2s - loss: 0.6906 - val_loss: 0.9187\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: val_loss did not improve\n",
      " - 2s - loss: 0.6907 - val_loss: 0.9198\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: val_loss did not improve\n",
      " - 2s - loss: 0.6905 - val_loss: 0.9192\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: val_loss did not improve\n",
      " - 2s - loss: 0.6907 - val_loss: 0.9193\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: val_loss did not improve\n",
      " - 2s - loss: 0.6905 - val_loss: 0.9186\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: val_loss did not improve\n",
      " - 2s - loss: 0.6905 - val_loss: 0.9190\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 00232: val_loss did not improve\n",
      " - 2s - loss: 0.6904 - val_loss: 0.9197\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 00233: val_loss did not improve\n",
      " - 2s - loss: 0.6905 - val_loss: 0.9197\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 00234: val_loss did not improve\n",
      " - 2s - loss: 0.6903 - val_loss: 0.9188\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 00235: val_loss did not improve\n",
      " - 2s - loss: 0.6903 - val_loss: 0.9196\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 00236: val_loss did not improve\n",
      " - 2s - loss: 0.6904 - val_loss: 0.9203\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 00237: val_loss did not improve\n",
      " - 2s - loss: 0.6903 - val_loss: 0.9204\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 00238: val_loss did not improve\n",
      " - 2s - loss: 0.6902 - val_loss: 0.9203\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 00239: val_loss did not improve\n",
      " - 2s - loss: 0.6901 - val_loss: 0.9206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/300\n",
      "\n",
      "Epoch 00240: val_loss did not improve\n",
      " - 2s - loss: 0.6903 - val_loss: 0.9219\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 00241: val_loss did not improve\n",
      " - 2s - loss: 0.6901 - val_loss: 0.9215\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 00242: val_loss did not improve\n",
      " - 2s - loss: 0.6902 - val_loss: 0.9224\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 00243: val_loss did not improve\n",
      " - 2s - loss: 0.6899 - val_loss: 0.9211\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 00244: val_loss did not improve\n",
      " - 2s - loss: 0.6900 - val_loss: 0.9221\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 00245: val_loss did not improve\n",
      " - 2s - loss: 0.6900 - val_loss: 0.9213\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 00246: val_loss did not improve\n",
      " - 2s - loss: 0.6899 - val_loss: 0.9219\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 00247: val_loss did not improve\n",
      " - 2s - loss: 0.6898 - val_loss: 0.9235\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 00248: val_loss did not improve\n",
      " - 2s - loss: 0.6898 - val_loss: 0.9233\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 00249: val_loss did not improve\n",
      " - 2s - loss: 0.6898 - val_loss: 0.9224\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 00250: val_loss did not improve\n",
      " - 2s - loss: 0.6898 - val_loss: 0.9241\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 00251: val_loss did not improve\n",
      " - 2s - loss: 0.6896 - val_loss: 0.9246\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 00252: val_loss did not improve\n",
      " - 2s - loss: 0.6896 - val_loss: 0.9251\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 00253: val_loss did not improve\n",
      " - 2s - loss: 0.6897 - val_loss: 0.9251\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 00254: val_loss did not improve\n",
      " - 2s - loss: 0.6896 - val_loss: 0.9252\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 00255: val_loss did not improve\n",
      " - 2s - loss: 0.6895 - val_loss: 0.9240\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 00256: val_loss did not improve\n",
      " - 2s - loss: 0.6896 - val_loss: 0.9255\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 00257: val_loss did not improve\n",
      " - 3s - loss: 0.6895 - val_loss: 0.9251\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 00258: val_loss did not improve\n",
      " - 2s - loss: 0.6894 - val_loss: 0.9248\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 00259: val_loss did not improve\n",
      " - 2s - loss: 0.6895 - val_loss: 0.9253\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 00260: val_loss did not improve\n",
      " - 2s - loss: 0.6893 - val_loss: 0.9259\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 00261: val_loss did not improve\n",
      " - 2s - loss: 0.6894 - val_loss: 0.9264\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 00262: val_loss did not improve\n",
      " - 2s - loss: 0.6893 - val_loss: 0.9267\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 00263: val_loss did not improve\n",
      " - 2s - loss: 0.6892 - val_loss: 0.9260\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 00264: val_loss did not improve\n",
      " - 2s - loss: 0.6892 - val_loss: 0.9263\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 00265: val_loss did not improve\n",
      " - 2s - loss: 0.6892 - val_loss: 0.9265\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 00266: val_loss did not improve\n",
      " - 2s - loss: 0.6892 - val_loss: 0.9260\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 00267: val_loss did not improve\n",
      " - 2s - loss: 0.6891 - val_loss: 0.9267\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 00268: val_loss did not improve\n",
      " - 2s - loss: 0.6891 - val_loss: 0.9268\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 00269: val_loss did not improve\n",
      " - 2s - loss: 0.6892 - val_loss: 0.9271\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 00270: val_loss did not improve\n",
      " - 2s - loss: 0.6891 - val_loss: 0.9274\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 00271: val_loss did not improve\n",
      " - 2s - loss: 0.6889 - val_loss: 0.9277\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 00272: val_loss did not improve\n",
      " - 2s - loss: 0.6891 - val_loss: 0.9270\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 00273: val_loss did not improve\n",
      " - 2s - loss: 0.6891 - val_loss: 0.9277\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 00274: val_loss did not improve\n",
      " - 2s - loss: 0.6888 - val_loss: 0.9287\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 00275: val_loss did not improve\n",
      " - 2s - loss: 0.6890 - val_loss: 0.9282\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 00276: val_loss did not improve\n",
      " - 2s - loss: 0.6890 - val_loss: 0.9294\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 00277: val_loss did not improve\n",
      " - 2s - loss: 0.6888 - val_loss: 0.9278\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 00278: val_loss did not improve\n",
      " - 2s - loss: 0.6889 - val_loss: 0.9287\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 00279: val_loss did not improve\n",
      " - 2s - loss: 0.6889 - val_loss: 0.9288\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 00280: val_loss did not improve\n",
      " - 2s - loss: 0.6888 - val_loss: 0.9302\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 00281: val_loss did not improve\n",
      " - 3s - loss: 0.6887 - val_loss: 0.9299\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 00282: val_loss did not improve\n",
      " - 2s - loss: 0.6887 - val_loss: 0.9300\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 00283: val_loss did not improve\n",
      " - 2s - loss: 0.6887 - val_loss: 0.9308\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 00284: val_loss did not improve\n",
      " - 2s - loss: 0.6888 - val_loss: 0.9308\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 00285: val_loss did not improve\n",
      " - 3s - loss: 0.6888 - val_loss: 0.9294\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 00286: val_loss did not improve\n",
      " - 2s - loss: 0.6885 - val_loss: 0.9291\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 00287: val_loss did not improve\n",
      " - 2s - loss: 0.6885 - val_loss: 0.9293\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 00288: val_loss did not improve\n",
      " - 2s - loss: 0.6886 - val_loss: 0.9302\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 00289: val_loss did not improve\n",
      " - 2s - loss: 0.6886 - val_loss: 0.9311\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 00290: val_loss did not improve\n",
      " - 2s - loss: 0.6885 - val_loss: 0.9314\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 00291: val_loss did not improve\n",
      " - 2s - loss: 0.6884 - val_loss: 0.9305\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 00292: val_loss did not improve\n",
      " - 2s - loss: 0.6885 - val_loss: 0.9309\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 00293: val_loss did not improve\n",
      " - 2s - loss: 0.6885 - val_loss: 0.9317\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 00294: val_loss did not improve\n",
      " - 2s - loss: 0.6886 - val_loss: 0.9321\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 00295: val_loss did not improve\n",
      " - 2s - loss: 0.6884 - val_loss: 0.9333\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 00296: val_loss did not improve\n",
      " - 2s - loss: 0.6884 - val_loss: 0.9331\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 00297: val_loss did not improve\n",
      " - 2s - loss: 0.6883 - val_loss: 0.9330\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 00298: val_loss did not improve\n",
      " - 2s - loss: 0.6882 - val_loss: 0.9325\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 00299: val_loss did not improve\n",
      " - 2s - loss: 0.6884 - val_loss: 0.9325\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 00300: val_loss did not improve\n",
      " - 2s - loss: 0.6882 - val_loss: 0.9331\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit([train.user_id, train.item_id], train.rating,\n",
    "                    batch_size=64, epochs=300, validation_split=0.1,\n",
    "                    shuffle=True, verbose=2, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwJgg0AFgTPw/7+BzVbEX5DkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss history\n",
    "losshist = history.history['loss']\n",
    "plt.plot(losshist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error 0.9362030419991877\n",
      "Root Mean Square Error 0.9765756499114648\n"
     ]
    }
   ],
   "source": [
    "# load the best model's parameters\n",
    "model.load_weights(\"weights.hdf5\")\n",
    "\n",
    "# predict the ratings\n",
    "y_pred = model.predict([test.user_id, test.item_id])\n",
    "rmse_factorize = np.sqrt(mean_squared_error(test.rating, y_pred))\n",
    "rmse_factorize_round = np.sqrt(mean_squared_error(test.rating, np.round(y_pred)))\n",
    "\n",
    "# Print out the evaluation result\n",
    "print(\"Root Mean Square Error {}\".format(rmse_factorize))\n",
    "print(\"Root Mean Square Error {}\".format(rmse_factorize_round))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.constraints import non_neg\n",
    "movie_input = Input(shape=[1],name='Item')\n",
    "movie_embedding = Embedding(NumItems + 1, n_latent_factors, name='NonNegMovie-Embedding',\n",
    "                                        embeddings_constraint=non_neg())(movie_input)\n",
    "movie_vec = Flatten(name='FlattenMovies')(movie_embedding)\n",
    "\n",
    "user_input = Input(shape=[1],name='User')\n",
    "user_embedding = Embedding(NumUsers + 1, n_latent_factors,name='NonNegUser-Embedding',\n",
    "                                        embeddings_constraint=non_neg())(user_input)\n",
    "user_vec = Flatten(name='FlattenUsers')(user_embedding)\n",
    "\n",
    "prod = dot([movie_vec, user_vec], axes=1, name='DotProduct')\n",
    "model = Model([user_input, movie_input], prod)\n",
    "model.compile('adam', 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.39103, saving model to weights_nonneg.hdf5\n",
      " - 4s - loss: 11.2814 - val_loss: 7.3910\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: val_loss improved from 7.39103 to 2.66735, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 4.4185 - val_loss: 2.6674\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.66735 to 1.62401, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 1.9998 - val_loss: 1.6240\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.62401 to 1.25216, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 1.3666 - val_loss: 1.2522\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.25216 to 1.09030, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 1.1163 - val_loss: 1.0903\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.09030 to 1.01256, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 1.0035 - val_loss: 1.0126\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.01256 to 0.97336, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.9482 - val_loss: 0.9734\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.97336 to 0.94897, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.9182 - val_loss: 0.9490\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.94897 to 0.93570, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.9009 - val_loss: 0.9357\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.93570 to 0.92819, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8901 - val_loss: 0.9282\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.92819 to 0.91996, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8829 - val_loss: 0.9200\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.91996 to 0.91513, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8777 - val_loss: 0.9151\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.91513 to 0.91098, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8738 - val_loss: 0.9110\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.91098 to 0.90793, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8710 - val_loss: 0.9079\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.90793 to 0.90580, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8686 - val_loss: 0.9058\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.90580 to 0.90359, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8669 - val_loss: 0.9036\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 3s - loss: 0.8657 - val_loss: 0.9037\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 3s - loss: 0.8640 - val_loss: 0.9038\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.90359 to 0.89973, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8633 - val_loss: 0.8997\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 3s - loss: 0.8623 - val_loss: 0.9004\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 3s - loss: 0.8617 - val_loss: 0.9002\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.89973 to 0.89883, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8610 - val_loss: 0.8988\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 3s - loss: 0.8608 - val_loss: 0.8995\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.89883 to 0.89851, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8600 - val_loss: 0.8985\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 3s - loss: 0.8595 - val_loss: 0.8995\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 3s - loss: 0.8591 - val_loss: 0.9017\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 3s - loss: 0.8587 - val_loss: 0.9013\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 3s - loss: 0.8583 - val_loss: 0.9008\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.89851 to 0.89695, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8580 - val_loss: 0.8970\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 3s - loss: 0.8577 - val_loss: 0.8992\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      " - 3s - loss: 0.8572 - val_loss: 0.8980\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 3s - loss: 0.8573 - val_loss: 0.8976\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      " - 3s - loss: 0.8568 - val_loss: 0.8996\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      " - 3s - loss: 0.8564 - val_loss: 0.8998\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      " - 3s - loss: 0.8559 - val_loss: 0.9000\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      " - 3s - loss: 0.8559 - val_loss: 0.8995\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      " - 3s - loss: 0.8553 - val_loss: 0.8988\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      " - 3s - loss: 0.8546 - val_loss: 0.8995\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      " - 3s - loss: 0.8544 - val_loss: 0.8995\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      " - 3s - loss: 0.8535 - val_loss: 0.8994\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      " - 3s - loss: 0.8528 - val_loss: 0.8987\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      " - 3s - loss: 0.8521 - val_loss: 0.8982\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      " - 3s - loss: 0.8509 - val_loss: 0.8973\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      " - 3s - loss: 0.8502 - val_loss: 0.8977\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.89695 to 0.89539, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8484 - val_loss: 0.8954\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.89539 to 0.89531, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8471 - val_loss: 0.8953\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.89531 to 0.89521, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8452 - val_loss: 0.8952\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.89521 to 0.89414, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8435 - val_loss: 0.8941\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.89414 to 0.89299, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8413 - val_loss: 0.8930\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      " - 3s - loss: 0.8382 - val_loss: 0.8935\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.89299 to 0.89063, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8360 - val_loss: 0.8906\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.89063 to 0.88971, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8325 - val_loss: 0.8897\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      " - 3s - loss: 0.8295 - val_loss: 0.8900\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.88971 to 0.88815, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8253 - val_loss: 0.8882\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.88815 to 0.88714, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8210 - val_loss: 0.8871\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.88714 to 0.88550, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8173 - val_loss: 0.8855\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.88550 to 0.88473, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8132 - val_loss: 0.8847\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.88473 to 0.88398, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8086 - val_loss: 0.8840\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.88398 to 0.88239, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.8046 - val_loss: 0.8824\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      " - 3s - loss: 0.8001 - val_loss: 0.8832\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      " - 3s - loss: 0.7957 - val_loss: 0.8834\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.88239 to 0.88084, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.7914 - val_loss: 0.8808\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      " - 3s - loss: 0.7878 - val_loss: 0.8815\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      " - 3s - loss: 0.7838 - val_loss: 0.8834\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      " - 3s - loss: 0.7801 - val_loss: 0.8817\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.88084 to 0.87967, saving model to weights_nonneg.hdf5\n",
      " - 3s - loss: 0.7769 - val_loss: 0.8797\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      " - 3s - loss: 0.7734 - val_loss: 0.8833\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      " - 3s - loss: 0.7704 - val_loss: 0.8823\n",
      "Epoch 69/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      " - 3s - loss: 0.7676 - val_loss: 0.8818\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      " - 3s - loss: 0.7649 - val_loss: 0.8830\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      " - 3s - loss: 0.7625 - val_loss: 0.8849\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      " - 3s - loss: 0.7602 - val_loss: 0.8843\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      " - 3s - loss: 0.7576 - val_loss: 0.8835\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      " - 3s - loss: 0.7557 - val_loss: 0.8845\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      " - 3s - loss: 0.7537 - val_loss: 0.8859\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      " - 3s - loss: 0.7514 - val_loss: 0.8859\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      " - 3s - loss: 0.7502 - val_loss: 0.8838\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      " - 3s - loss: 0.7484 - val_loss: 0.8887\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      " - 3s - loss: 0.7468 - val_loss: 0.8868\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      " - 3s - loss: 0.7453 - val_loss: 0.8877\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      " - 3s - loss: 0.7440 - val_loss: 0.8874\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      " - 3s - loss: 0.7428 - val_loss: 0.8896\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      " - 3s - loss: 0.7414 - val_loss: 0.8884\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      " - 3s - loss: 0.7403 - val_loss: 0.8891\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      " - 3s - loss: 0.7392 - val_loss: 0.8898\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      " - 3s - loss: 0.7380 - val_loss: 0.8898\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      " - 3s - loss: 0.7370 - val_loss: 0.8896\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      " - 3s - loss: 0.7361 - val_loss: 0.8893\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      " - 3s - loss: 0.7352 - val_loss: 0.8888\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      " - 3s - loss: 0.7345 - val_loss: 0.8892\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      " - 3s - loss: 0.7339 - val_loss: 0.8904\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      " - 3s - loss: 0.7328 - val_loss: 0.8900\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      " - 3s - loss: 0.7319 - val_loss: 0.8929\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      " - 3s - loss: 0.7314 - val_loss: 0.8934\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      " - 3s - loss: 0.7308 - val_loss: 0.8932\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      " - 3s - loss: 0.7302 - val_loss: 0.8922\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      " - 3s - loss: 0.7293 - val_loss: 0.8944\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      " - 3s - loss: 0.7289 - val_loss: 0.8953\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      " - 3s - loss: 0.7285 - val_loss: 0.8937\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      " - 3s - loss: 0.7277 - val_loss: 0.8941\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      " - 3s - loss: 0.7272 - val_loss: 0.8936\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: val_loss did not improve\n",
      " - 3s - loss: 0.7268 - val_loss: 0.8961\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: val_loss did not improve\n",
      " - 3s - loss: 0.7263 - val_loss: 0.8948\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      " - 3s - loss: 0.7258 - val_loss: 0.8957\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      " - 3s - loss: 0.7251 - val_loss: 0.8953\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      " - 3s - loss: 0.7251 - val_loss: 0.8952\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: val_loss did not improve\n",
      " - 3s - loss: 0.7246 - val_loss: 0.8954\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: val_loss did not improve\n",
      " - 3s - loss: 0.7243 - val_loss: 0.8942\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: val_loss did not improve\n",
      " - 3s - loss: 0.7238 - val_loss: 0.8950\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: val_loss did not improve\n",
      " - 3s - loss: 0.7236 - val_loss: 0.8960\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: val_loss did not improve\n",
      " - 3s - loss: 0.7229 - val_loss: 0.8959\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: val_loss did not improve\n",
      " - 3s - loss: 0.7227 - val_loss: 0.8980\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: val_loss did not improve\n",
      " - 3s - loss: 0.7222 - val_loss: 0.8976\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: val_loss did not improve\n",
      " - 3s - loss: 0.7219 - val_loss: 0.8976\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: val_loss did not improve\n",
      " - 3s - loss: 0.7217 - val_loss: 0.8972\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: val_loss did not improve\n",
      " - 3s - loss: 0.7218 - val_loss: 0.8967\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      " - 3s - loss: 0.7211 - val_loss: 0.8972\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: val_loss did not improve\n",
      " - 3s - loss: 0.7208 - val_loss: 0.8963\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: val_loss did not improve\n",
      " - 3s - loss: 0.7206 - val_loss: 0.8994\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      " - 3s - loss: 0.7201 - val_loss: 0.8983\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: val_loss did not improve\n",
      " - 3s - loss: 0.7200 - val_loss: 0.8977\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: val_loss did not improve\n",
      " - 3s - loss: 0.7199 - val_loss: 0.8979\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: val_loss did not improve\n",
      " - 3s - loss: 0.7192 - val_loss: 0.8964\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: val_loss did not improve\n",
      " - 3s - loss: 0.7190 - val_loss: 0.8984\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: val_loss did not improve\n",
      " - 3s - loss: 0.7191 - val_loss: 0.8993\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      " - 3s - loss: 0.7190 - val_loss: 0.8984\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      " - 3s - loss: 0.7183 - val_loss: 0.8971\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: val_loss did not improve\n",
      " - 3s - loss: 0.7182 - val_loss: 0.8981\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      " - 3s - loss: 0.7181 - val_loss: 0.8977\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      " - 3s - loss: 0.7179 - val_loss: 0.8980\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: val_loss did not improve\n",
      " - 3s - loss: 0.7176 - val_loss: 0.8980\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      " - 3s - loss: 0.7177 - val_loss: 0.8980\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: val_loss did not improve\n",
      " - 3s - loss: 0.7174 - val_loss: 0.8967\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: val_loss did not improve\n",
      " - 3s - loss: 0.7171 - val_loss: 0.8991\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: val_loss did not improve\n",
      " - 3s - loss: 0.7169 - val_loss: 0.9008\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: val_loss did not improve\n",
      " - 3s - loss: 0.7166 - val_loss: 0.9003\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: val_loss did not improve\n",
      " - 3s - loss: 0.7167 - val_loss: 0.9008\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: val_loss did not improve\n",
      " - 3s - loss: 0.7163 - val_loss: 0.8993\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: val_loss did not improve\n",
      " - 3s - loss: 0.7162 - val_loss: 0.8972\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: val_loss did not improve\n",
      " - 3s - loss: 0.7160 - val_loss: 0.9012\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: val_loss did not improve\n",
      " - 3s - loss: 0.7161 - val_loss: 0.8986\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: val_loss did not improve\n",
      " - 3s - loss: 0.7157 - val_loss: 0.8977\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: val_loss did not improve\n",
      " - 3s - loss: 0.7158 - val_loss: 0.9000\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: val_loss did not improve\n",
      " - 3s - loss: 0.7154 - val_loss: 0.9000\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: val_loss did not improve\n",
      " - 3s - loss: 0.7155 - val_loss: 0.8991\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: val_loss did not improve\n",
      " - 3s - loss: 0.7152 - val_loss: 0.8989\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: val_loss did not improve\n",
      " - 3s - loss: 0.7151 - val_loss: 0.8993\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: val_loss did not improve\n",
      " - 3s - loss: 0.7147 - val_loss: 0.8995\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: val_loss did not improve\n",
      " - 3s - loss: 0.7149 - val_loss: 0.9011\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: val_loss did not improve\n",
      " - 3s - loss: 0.7146 - val_loss: 0.9001\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: val_loss did not improve\n",
      " - 3s - loss: 0.7141 - val_loss: 0.9010\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: val_loss did not improve\n",
      " - 3s - loss: 0.7146 - val_loss: 0.9003\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: val_loss did not improve\n",
      " - 3s - loss: 0.7142 - val_loss: 0.8985\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: val_loss did not improve\n",
      " - 3s - loss: 0.7138 - val_loss: 0.9016\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: val_loss did not improve\n",
      " - 3s - loss: 0.7143 - val_loss: 0.9012\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: val_loss did not improve\n",
      " - 3s - loss: 0.7138 - val_loss: 0.9015\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: val_loss did not improve\n",
      " - 3s - loss: 0.7139 - val_loss: 0.9005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: val_loss did not improve\n",
      " - 3s - loss: 0.7135 - val_loss: 0.8997\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: val_loss did not improve\n",
      " - 3s - loss: 0.7137 - val_loss: 0.9017\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: val_loss did not improve\n",
      " - 3s - loss: 0.7135 - val_loss: 0.9017\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: val_loss did not improve\n",
      " - 3s - loss: 0.7135 - val_loss: 0.9013\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: val_loss did not improve\n",
      " - 3s - loss: 0.7133 - val_loss: 0.9033\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: val_loss did not improve\n",
      " - 3s - loss: 0.7131 - val_loss: 0.9023\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: val_loss did not improve\n",
      " - 3s - loss: 0.7133 - val_loss: 0.9018\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: val_loss did not improve\n",
      " - 3s - loss: 0.7128 - val_loss: 0.9015\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: val_loss did not improve\n",
      " - 3s - loss: 0.7128 - val_loss: 0.9013\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: val_loss did not improve\n",
      " - 3s - loss: 0.7128 - val_loss: 0.9008\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: val_loss did not improve\n",
      " - 3s - loss: 0.7126 - val_loss: 0.9010\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: val_loss did not improve\n",
      " - 3s - loss: 0.7124 - val_loss: 0.9032\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: val_loss did not improve\n",
      " - 3s - loss: 0.7122 - val_loss: 0.9030\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: val_loss did not improve\n",
      " - 3s - loss: 0.7125 - val_loss: 0.9031\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: val_loss did not improve\n",
      " - 3s - loss: 0.7126 - val_loss: 0.9037\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: val_loss did not improve\n",
      " - 3s - loss: 0.7121 - val_loss: 0.9016\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: val_loss did not improve\n",
      " - 3s - loss: 0.7119 - val_loss: 0.9031\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: val_loss did not improve\n",
      " - 3s - loss: 0.7121 - val_loss: 0.9023\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: val_loss did not improve\n",
      " - 3s - loss: 0.7121 - val_loss: 0.9037\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: val_loss did not improve\n",
      " - 3s - loss: 0.7116 - val_loss: 0.9009\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: val_loss did not improve\n",
      " - 3s - loss: 0.7118 - val_loss: 0.9028\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: val_loss did not improve\n",
      " - 3s - loss: 0.7117 - val_loss: 0.9030\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: val_loss did not improve\n",
      " - 3s - loss: 0.7117 - val_loss: 0.9037\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: val_loss did not improve\n",
      " - 3s - loss: 0.7114 - val_loss: 0.9024\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: val_loss did not improve\n",
      " - 3s - loss: 0.7116 - val_loss: 0.9032\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: val_loss did not improve\n",
      " - 3s - loss: 0.7114 - val_loss: 0.9020\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: val_loss did not improve\n",
      " - 3s - loss: 0.7113 - val_loss: 0.9019\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: val_loss did not improve\n",
      " - 3s - loss: 0.7115 - val_loss: 0.9010\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: val_loss did not improve\n",
      " - 3s - loss: 0.7111 - val_loss: 0.9028\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: val_loss did not improve\n",
      " - 3s - loss: 0.7112 - val_loss: 0.9026\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: val_loss did not improve\n",
      " - 3s - loss: 0.7109 - val_loss: 0.9034\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: val_loss did not improve\n",
      " - 3s - loss: 0.7109 - val_loss: 0.9037\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: val_loss did not improve\n",
      " - 3s - loss: 0.7110 - val_loss: 0.9025\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: val_loss did not improve\n",
      " - 3s - loss: 0.7107 - val_loss: 0.9024\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: val_loss did not improve\n",
      " - 3s - loss: 0.7109 - val_loss: 0.9036\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: val_loss did not improve\n",
      " - 3s - loss: 0.7107 - val_loss: 0.9023\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: val_loss did not improve\n",
      " - 3s - loss: 0.7108 - val_loss: 0.9019\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: val_loss did not improve\n",
      " - 3s - loss: 0.7105 - val_loss: 0.9026\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: val_loss did not improve\n",
      " - 3s - loss: 0.7104 - val_loss: 0.9012\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: val_loss did not improve\n",
      " - 3s - loss: 0.7104 - val_loss: 0.9048\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: val_loss did not improve\n",
      " - 3s - loss: 0.7103 - val_loss: 0.9036\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: val_loss did not improve\n",
      " - 3s - loss: 0.7104 - val_loss: 0.9026\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: val_loss did not improve\n",
      " - 3s - loss: 0.7103 - val_loss: 0.9034\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: val_loss did not improve\n",
      " - 3s - loss: 0.7100 - val_loss: 0.9032\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: val_loss did not improve\n",
      " - 3s - loss: 0.7103 - val_loss: 0.9040\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: val_loss did not improve\n",
      " - 3s - loss: 0.7102 - val_loss: 0.9025\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: val_loss did not improve\n",
      " - 3s - loss: 0.7098 - val_loss: 0.9048\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: val_loss did not improve\n",
      " - 3s - loss: 0.7101 - val_loss: 0.9035\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: val_loss did not improve\n",
      " - 3s - loss: 0.7099 - val_loss: 0.9021\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: val_loss did not improve\n",
      " - 3s - loss: 0.7100 - val_loss: 0.9024\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: val_loss did not improve\n",
      " - 3s - loss: 0.7098 - val_loss: 0.9024\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: val_loss did not improve\n",
      " - 3s - loss: 0.7098 - val_loss: 0.9015\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: val_loss did not improve\n",
      " - 3s - loss: 0.7099 - val_loss: 0.9033\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: val_loss did not improve\n",
      " - 3s - loss: 0.7097 - val_loss: 0.9020\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: val_loss did not improve\n",
      " - 3s - loss: 0.7096 - val_loss: 0.9037\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: val_loss did not improve\n",
      " - 3s - loss: 0.7096 - val_loss: 0.9028\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: val_loss did not improve\n",
      " - 3s - loss: 0.7097 - val_loss: 0.9056\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: val_loss did not improve\n",
      " - 3s - loss: 0.7095 - val_loss: 0.9032\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: val_loss did not improve\n",
      " - 3s - loss: 0.7095 - val_loss: 0.9036\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: val_loss did not improve\n",
      " - 3s - loss: 0.7092 - val_loss: 0.9024\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: val_loss did not improve\n",
      " - 3s - loss: 0.7094 - val_loss: 0.9045\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: val_loss did not improve\n",
      " - 3s - loss: 0.7093 - val_loss: 0.9017\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: val_loss did not improve\n",
      " - 3s - loss: 0.7091 - val_loss: 0.9044\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: val_loss did not improve\n",
      " - 3s - loss: 0.7092 - val_loss: 0.9044\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: val_loss did not improve\n",
      " - 3s - loss: 0.7095 - val_loss: 0.9054\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: val_loss did not improve\n",
      " - 3s - loss: 0.7090 - val_loss: 0.9046\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: val_loss did not improve\n",
      " - 3s - loss: 0.7087 - val_loss: 0.9048\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: val_loss did not improve\n",
      " - 3s - loss: 0.7091 - val_loss: 0.9033\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: val_loss did not improve\n",
      " - 3s - loss: 0.7090 - val_loss: 0.9042\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: val_loss did not improve\n",
      " - 3s - loss: 0.7090 - val_loss: 0.9034\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: val_loss did not improve\n",
      " - 3s - loss: 0.7087 - val_loss: 0.9049\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: val_loss did not improve\n",
      " - 3s - loss: 0.7088 - val_loss: 0.9048\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: val_loss did not improve\n",
      " - 3s - loss: 0.7089 - val_loss: 0.9051\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: val_loss did not improve\n",
      " - 3s - loss: 0.7090 - val_loss: 0.9043\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 00232: val_loss did not improve\n",
      " - 3s - loss: 0.7088 - val_loss: 0.9024\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 00233: val_loss did not improve\n",
      " - 3s - loss: 0.7086 - val_loss: 0.9060\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 00234: val_loss did not improve\n",
      " - 3s - loss: 0.7087 - val_loss: 0.9050\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 00235: val_loss did not improve\n",
      " - 3s - loss: 0.7086 - val_loss: 0.9031\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 00236: val_loss did not improve\n",
      " - 3s - loss: 0.7088 - val_loss: 0.9037\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 00237: val_loss did not improve\n",
      " - 3s - loss: 0.7084 - val_loss: 0.9047\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 00238: val_loss did not improve\n",
      " - 3s - loss: 0.7085 - val_loss: 0.9043\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 00239: val_loss did not improve\n",
      " - 3s - loss: 0.7085 - val_loss: 0.9068\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 00240: val_loss did not improve\n",
      " - 3s - loss: 0.7084 - val_loss: 0.9039\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 00241: val_loss did not improve\n",
      " - 3s - loss: 0.7080 - val_loss: 0.9050\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 00242: val_loss did not improve\n",
      " - 3s - loss: 0.7084 - val_loss: 0.9046\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 00243: val_loss did not improve\n",
      " - 3s - loss: 0.7083 - val_loss: 0.9059\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 00244: val_loss did not improve\n",
      " - 3s - loss: 0.7084 - val_loss: 0.9051\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 00245: val_loss did not improve\n",
      " - 3s - loss: 0.7081 - val_loss: 0.9040\n",
      "Epoch 246/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00246: val_loss did not improve\n",
      " - 3s - loss: 0.7083 - val_loss: 0.9043\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 00247: val_loss did not improve\n",
      " - 3s - loss: 0.7082 - val_loss: 0.9044\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 00248: val_loss did not improve\n",
      " - 3s - loss: 0.7080 - val_loss: 0.9047\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 00249: val_loss did not improve\n",
      " - 3s - loss: 0.7079 - val_loss: 0.9055\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 00250: val_loss did not improve\n",
      " - 3s - loss: 0.7081 - val_loss: 0.9055\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 00251: val_loss did not improve\n",
      " - 3s - loss: 0.7081 - val_loss: 0.9043\n",
      "Epoch 252/300\n",
      "\n",
      "Epoch 00252: val_loss did not improve\n",
      " - 3s - loss: 0.7080 - val_loss: 0.9058\n",
      "Epoch 253/300\n",
      "\n",
      "Epoch 00253: val_loss did not improve\n",
      " - 3s - loss: 0.7078 - val_loss: 0.9051\n",
      "Epoch 254/300\n",
      "\n",
      "Epoch 00254: val_loss did not improve\n",
      " - 3s - loss: 0.7079 - val_loss: 0.9061\n",
      "Epoch 255/300\n",
      "\n",
      "Epoch 00255: val_loss did not improve\n",
      " - 3s - loss: 0.7077 - val_loss: 0.9037\n",
      "Epoch 256/300\n",
      "\n",
      "Epoch 00256: val_loss did not improve\n",
      " - 3s - loss: 0.7080 - val_loss: 0.9050\n",
      "Epoch 257/300\n",
      "\n",
      "Epoch 00257: val_loss did not improve\n",
      " - 3s - loss: 0.7075 - val_loss: 0.9064\n",
      "Epoch 258/300\n",
      "\n",
      "Epoch 00258: val_loss did not improve\n",
      " - 3s - loss: 0.7079 - val_loss: 0.9051\n",
      "Epoch 259/300\n",
      "\n",
      "Epoch 00259: val_loss did not improve\n",
      " - 3s - loss: 0.7077 - val_loss: 0.9052\n",
      "Epoch 260/300\n",
      "\n",
      "Epoch 00260: val_loss did not improve\n",
      " - 3s - loss: 0.7078 - val_loss: 0.9052\n",
      "Epoch 261/300\n",
      "\n",
      "Epoch 00261: val_loss did not improve\n",
      " - 3s - loss: 0.7076 - val_loss: 0.9047\n",
      "Epoch 262/300\n",
      "\n",
      "Epoch 00262: val_loss did not improve\n",
      " - 3s - loss: 0.7075 - val_loss: 0.9056\n",
      "Epoch 263/300\n",
      "\n",
      "Epoch 00263: val_loss did not improve\n",
      " - 3s - loss: 0.7077 - val_loss: 0.9038\n",
      "Epoch 264/300\n",
      "\n",
      "Epoch 00264: val_loss did not improve\n",
      " - 3s - loss: 0.7075 - val_loss: 0.9044\n",
      "Epoch 265/300\n",
      "\n",
      "Epoch 00265: val_loss did not improve\n",
      " - 3s - loss: 0.7074 - val_loss: 0.9052\n",
      "Epoch 266/300\n",
      "\n",
      "Epoch 00266: val_loss did not improve\n",
      " - 3s - loss: 0.7074 - val_loss: 0.9044\n",
      "Epoch 267/300\n",
      "\n",
      "Epoch 00267: val_loss did not improve\n",
      " - 3s - loss: 0.7074 - val_loss: 0.9049\n",
      "Epoch 268/300\n",
      "\n",
      "Epoch 00268: val_loss did not improve\n",
      " - 3s - loss: 0.7073 - val_loss: 0.9041\n",
      "Epoch 269/300\n",
      "\n",
      "Epoch 00269: val_loss did not improve\n",
      " - 3s - loss: 0.7074 - val_loss: 0.9059\n",
      "Epoch 270/300\n",
      "\n",
      "Epoch 00270: val_loss did not improve\n",
      " - 3s - loss: 0.7076 - val_loss: 0.9058\n",
      "Epoch 271/300\n",
      "\n",
      "Epoch 00271: val_loss did not improve\n",
      " - 3s - loss: 0.7075 - val_loss: 0.9049\n",
      "Epoch 272/300\n",
      "\n",
      "Epoch 00272: val_loss did not improve\n",
      " - 3s - loss: 0.7074 - val_loss: 0.9029\n",
      "Epoch 273/300\n",
      "\n",
      "Epoch 00273: val_loss did not improve\n",
      " - 3s - loss: 0.7070 - val_loss: 0.9053\n",
      "Epoch 274/300\n",
      "\n",
      "Epoch 00274: val_loss did not improve\n",
      " - 3s - loss: 0.7072 - val_loss: 0.9054\n",
      "Epoch 275/300\n",
      "\n",
      "Epoch 00275: val_loss did not improve\n",
      " - 3s - loss: 0.7070 - val_loss: 0.9044\n",
      "Epoch 276/300\n",
      "\n",
      "Epoch 00276: val_loss did not improve\n",
      " - 3s - loss: 0.7073 - val_loss: 0.9066\n",
      "Epoch 277/300\n",
      "\n",
      "Epoch 00277: val_loss did not improve\n",
      " - 3s - loss: 0.7071 - val_loss: 0.9055\n",
      "Epoch 278/300\n",
      "\n",
      "Epoch 00278: val_loss did not improve\n",
      " - 3s - loss: 0.7071 - val_loss: 0.9044\n",
      "Epoch 279/300\n",
      "\n",
      "Epoch 00279: val_loss did not improve\n",
      " - 3s - loss: 0.7070 - val_loss: 0.9061\n",
      "Epoch 280/300\n",
      "\n",
      "Epoch 00280: val_loss did not improve\n",
      " - 3s - loss: 0.7070 - val_loss: 0.9084\n",
      "Epoch 281/300\n",
      "\n",
      "Epoch 00281: val_loss did not improve\n",
      " - 3s - loss: 0.7072 - val_loss: 0.9074\n",
      "Epoch 282/300\n",
      "\n",
      "Epoch 00282: val_loss did not improve\n",
      " - 3s - loss: 0.7069 - val_loss: 0.9075\n",
      "Epoch 283/300\n",
      "\n",
      "Epoch 00283: val_loss did not improve\n",
      " - 3s - loss: 0.7071 - val_loss: 0.9065\n",
      "Epoch 284/300\n",
      "\n",
      "Epoch 00284: val_loss did not improve\n",
      " - 3s - loss: 0.7071 - val_loss: 0.9053\n",
      "Epoch 285/300\n",
      "\n",
      "Epoch 00285: val_loss did not improve\n",
      " - 3s - loss: 0.7068 - val_loss: 0.9041\n",
      "Epoch 286/300\n",
      "\n",
      "Epoch 00286: val_loss did not improve\n",
      " - 3s - loss: 0.7067 - val_loss: 0.9076\n",
      "Epoch 287/300\n",
      "\n",
      "Epoch 00287: val_loss did not improve\n",
      " - 3s - loss: 0.7068 - val_loss: 0.9056\n",
      "Epoch 288/300\n",
      "\n",
      "Epoch 00288: val_loss did not improve\n",
      " - 3s - loss: 0.7067 - val_loss: 0.9029\n",
      "Epoch 289/300\n",
      "\n",
      "Epoch 00289: val_loss did not improve\n",
      " - 3s - loss: 0.7068 - val_loss: 0.9069\n",
      "Epoch 290/300\n",
      "\n",
      "Epoch 00290: val_loss did not improve\n",
      " - 3s - loss: 0.7070 - val_loss: 0.9060\n",
      "Epoch 291/300\n",
      "\n",
      "Epoch 00291: val_loss did not improve\n",
      " - 3s - loss: 0.7069 - val_loss: 0.9054\n",
      "Epoch 292/300\n",
      "\n",
      "Epoch 00292: val_loss did not improve\n",
      " - 3s - loss: 0.7067 - val_loss: 0.9053\n",
      "Epoch 293/300\n",
      "\n",
      "Epoch 00293: val_loss did not improve\n",
      " - 3s - loss: 0.7068 - val_loss: 0.9053\n",
      "Epoch 294/300\n",
      "\n",
      "Epoch 00294: val_loss did not improve\n",
      " - 3s - loss: 0.7066 - val_loss: 0.9058\n",
      "Epoch 295/300\n",
      "\n",
      "Epoch 00295: val_loss did not improve\n",
      " - 3s - loss: 0.7067 - val_loss: 0.9065\n",
      "Epoch 296/300\n",
      "\n",
      "Epoch 00296: val_loss did not improve\n",
      " - 3s - loss: 0.7067 - val_loss: 0.9063\n",
      "Epoch 297/300\n",
      "\n",
      "Epoch 00297: val_loss did not improve\n",
      " - 3s - loss: 0.7065 - val_loss: 0.9068\n",
      "Epoch 298/300\n",
      "\n",
      "Epoch 00298: val_loss did not improve\n",
      " - 3s - loss: 0.7066 - val_loss: 0.9057\n",
      "Epoch 299/300\n",
      "\n",
      "Epoch 00299: val_loss did not improve\n",
      " - 3s - loss: 0.7066 - val_loss: 0.9059\n",
      "Epoch 300/300\n",
      "\n",
      "Epoch 00300: val_loss did not improve\n",
      " - 3s - loss: 0.7065 - val_loss: 0.9069\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"weights_nonneg.hdf5\", verbose=1, save_best_only=True)\n",
    "\n",
    "history_nonneg = model.fit([train.user_id, train.item_id], train.rating,\n",
    "                    epochs=300, validation_split=0.1,\n",
    "                    shuffle=True, verbose=2, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error 0.8895137984172732\n",
      "Root Mean Square Error (rounded) 0.9341841360245848\n"
     ]
    }
   ],
   "source": [
    "# load the best model's parameters\n",
    "model.load_weights(\"weights_nonneg.hdf5\")\n",
    "\n",
    "# predict the ratings\n",
    "y_pred_nonneg = model.predict([test.user_id, test.item_id])\n",
    "rmse_factorize = np.sqrt(mean_squared_error(test.rating, y_pred_nonneg))\n",
    "rmse_factorize_rounded = np.sqrt(mean_squared_error(test.rating, np.round(y_pred_nonneg)))\n",
    "\n",
    "# Print out the RMSE\n",
    "print(\"Root Mean Square Error {}\".format(rmse_factorize))\n",
    "print(\"Root Mean Square Error (rounded) {}\".format(rmse_factorize_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72000 samples, validate on 8000 samples\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.93636, saving model to weights_nn.hdf5\n",
      " - 4s - loss: 1.4068 - val_loss: 0.9364\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.93636 to 0.91417, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.9883 - val_loss: 0.9142\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 3s - loss: 0.9508 - val_loss: 0.9269\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.91417 to 0.90578, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.9479 - val_loss: 0.9058\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 3s - loss: 0.9500 - val_loss: 0.9674\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 3s - loss: 0.9580 - val_loss: 0.9226\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 3s - loss: 0.9456 - val_loss: 0.9238\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 3s - loss: 0.9428 - val_loss: 0.9268\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 3s - loss: 0.9436 - val_loss: 0.9139\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 3s - loss: 0.8933 - val_loss: 0.9242\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.90578 to 0.89934, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.8807 - val_loss: 0.8993\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.89934 to 0.89615, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.8752 - val_loss: 0.8962\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.89615 to 0.87986, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.8667 - val_loss: 0.8799\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 3s - loss: 0.8634 - val_loss: 0.8873\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 3s - loss: 0.8547 - val_loss: 0.8884\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 3s - loss: 0.8467 - val_loss: 0.8864\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 3s - loss: 0.8444 - val_loss: 0.8830\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 3s - loss: 0.8382 - val_loss: 0.8829\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 3s - loss: 0.8358 - val_loss: 0.8820\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.87986 to 0.86978, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.8023 - val_loss: 0.8698\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.86978 to 0.86613, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.7972 - val_loss: 0.8661\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 3s - loss: 0.7930 - val_loss: 0.8665\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      " - 3s - loss: 0.7879 - val_loss: 0.8669\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.86613 to 0.86041, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.7841 - val_loss: 0.8604\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 3s - loss: 0.7806 - val_loss: 0.8677\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.86041 to 0.85752, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.7803 - val_loss: 0.8575\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 3s - loss: 0.7769 - val_loss: 0.8628\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 3s - loss: 0.7724 - val_loss: 0.8600\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 3s - loss: 0.7716 - val_loss: 0.8736\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 3s - loss: 0.7538 - val_loss: 0.8580\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      " - 3s - loss: 0.7501 - val_loss: 0.8589\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 3s - loss: 0.7505 - val_loss: 0.8589\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      " - 3s - loss: 0.7445 - val_loss: 0.8620\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      " - 3s - loss: 0.7457 - val_loss: 0.8592\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      " - 3s - loss: 0.7401 - val_loss: 0.8643\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      " - 3s - loss: 0.7387 - val_loss: 0.8617\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      " - 3s - loss: 0.7423 - val_loss: 0.8623\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      " - 3s - loss: 0.7398 - val_loss: 0.8608\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      " - 3s - loss: 0.7348 - val_loss: 0.8618\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      " - 3s - loss: 0.7318 - val_loss: 0.8607\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      " - 3s - loss: 0.7281 - val_loss: 0.8622\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      " - 3s - loss: 0.7271 - val_loss: 0.8594\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      " - 3s - loss: 0.7277 - val_loss: 0.8587\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      " - 3s - loss: 0.7226 - val_loss: 0.8619\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      " - 3s - loss: 0.7209 - val_loss: 0.8591\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.85752 to 0.85719, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.7179 - val_loss: 0.8572\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      " - 3s - loss: 0.7210 - val_loss: 0.8579\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.85719 to 0.85659, saving model to weights_nn.hdf5\n",
      " - 3s - loss: 0.7225 - val_loss: 0.8566\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      " - 3s - loss: 0.7209 - val_loss: 0.8593\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      " - 3s - loss: 0.7150 - val_loss: 0.8595\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      " - 3s - loss: 0.7124 - val_loss: 0.8584\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      " - 3s - loss: 0.7133 - val_loss: 0.8603\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      " - 3s - loss: 0.7139 - val_loss: 0.8581\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      " - 3s - loss: 0.7156 - val_loss: 0.8594\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      " - 3s - loss: 0.7138 - val_loss: 0.8585\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      " - 3s - loss: 0.7117 - val_loss: 0.8593\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      " - 3s - loss: 0.7077 - val_loss: 0.8598\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      " - 3s - loss: 0.7113 - val_loss: 0.8596\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      " - 3s - loss: 0.7092 - val_loss: 0.8597\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      " - 3s - loss: 0.7107 - val_loss: 0.8581\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      " - 3s - loss: 0.7061 - val_loss: 0.8588\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      " - 3s - loss: 0.7086 - val_loss: 0.8583\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      " - 3s - loss: 0.7082 - val_loss: 0.8598\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      " - 4s - loss: 0.7067 - val_loss: 0.8596\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      " - 3s - loss: 0.7086 - val_loss: 0.8591\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      " - 3s - loss: 0.7037 - val_loss: 0.8601\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      " - 3s - loss: 0.7078 - val_loss: 0.8592\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      " - 3s - loss: 0.7029 - val_loss: 0.8587\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      " - 3s - loss: 0.7064 - val_loss: 0.8590\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      " - 3s - loss: 0.7031 - val_loss: 0.8586\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      " - 3s - loss: 0.7049 - val_loss: 0.8595\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      " - 3s - loss: 0.7038 - val_loss: 0.8599\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      " - 3s - loss: 0.7060 - val_loss: 0.8584\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      " - 3s - loss: 0.7037 - val_loss: 0.8591\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      " - 3s - loss: 0.7022 - val_loss: 0.8583\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      " - 3s - loss: 0.7041 - val_loss: 0.8585\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      " - 3s - loss: 0.7024 - val_loss: 0.8592\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      " - 3s - loss: 0.7023 - val_loss: 0.8599\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: val_loss did not improve\n",
      " - 3s - loss: 0.7015 - val_loss: 0.8591\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      " - 3s - loss: 0.7078 - val_loss: 0.8593\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      " - 3s - loss: 0.7037 - val_loss: 0.8595\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      " - 3s - loss: 0.7043 - val_loss: 0.8596\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      " - 3s - loss: 0.7037 - val_loss: 0.8592\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      " - 3s - loss: 0.7000 - val_loss: 0.8591\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      " - 3s - loss: 0.7046 - val_loss: 0.8590\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      " - 3s - loss: 0.7032 - val_loss: 0.8589\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: val_loss did not improve\n",
      " - 3s - loss: 0.7049 - val_loss: 0.8588\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      " - 3s - loss: 0.7022 - val_loss: 0.8591\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      " - 3s - loss: 0.7029 - val_loss: 0.8590\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      " - 3s - loss: 0.6997 - val_loss: 0.8590\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      " - 3s - loss: 0.7048 - val_loss: 0.8591\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      " - 3s - loss: 0.7010 - val_loss: 0.8589\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      " - 3s - loss: 0.7013 - val_loss: 0.8589\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      " - 3s - loss: 0.7030 - val_loss: 0.8591\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      " - 3s - loss: 0.7034 - val_loss: 0.8595\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      " - 3s - loss: 0.7016 - val_loss: 0.8593\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      " - 3s - loss: 0.7005 - val_loss: 0.8595\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      " - 3s - loss: 0.7003 - val_loss: 0.8593\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      " - 3s - loss: 0.7020 - val_loss: 0.8592\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      " - 3s - loss: 0.7051 - val_loss: 0.8592\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      " - 3s - loss: 0.6999 - val_loss: 0.8593\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: val_loss did not improve\n",
      " - 3s - loss: 0.7014 - val_loss: 0.8590\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: val_loss did not improve\n",
      " - 4s - loss: 0.7008 - val_loss: 0.8590\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      " - 3s - loss: 0.6993 - val_loss: 0.8590\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      " - 3s - loss: 0.7027 - val_loss: 0.8591\n",
      "Epoch 106/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-fa687f7d9fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m history = model_nn.fit([train.user_id, train.item_id], train.rating,\n\u001b[1;32m     39\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     shuffle=True, verbose=2, callbacks=[checkpointer, lrate])\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/pytorch/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.layers import concatenate, Dropout\n",
    "\n",
    "# For each sample we input the integer identifiers\n",
    "# of a single user and a single item\n",
    "user_id_input = Input(shape=[1], name='user')\n",
    "item_id_input = Input(shape=[1], name='item')\n",
    "\n",
    "embedding_size = 50\n",
    "user_embedding = Embedding(output_dim=embedding_size, input_dim=NumUsers + 1,\n",
    "                           input_length=1, name='User-Embedding')(user_id_input)\n",
    "item_embedding = Embedding(output_dim=embedding_size, input_dim=NumItems + 1,\n",
    "                           input_length=1, name='Movie-Embedding')(item_id_input)\n",
    "\n",
    "\n",
    "# reshape from shape: (batch_size, input_length, embedding_size)\n",
    "# to shape: (batch_size, input_length * embedding_size) which is\n",
    "# equal to shape: (batch_size, embedding_size)\n",
    "user_vecs = Flatten()(user_embedding)\n",
    "item_vecs = Flatten()(item_embedding)\n",
    "input_vecs = concatenate([user_vecs, item_vecs])\n",
    "\n",
    "input_vecs = Dropout(0.2)(input_vecs)\n",
    "x1 = Dense(64, kernel_initializer='normal', activation='relu')(input_vecs)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "x2 = Dense(32, kernel_initializer='normal', activation='relu')(x1)\n",
    "y = Dense(1)(x1)\n",
    "model_nn = Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "\n",
    "## A binary crossentropy loss is only useful for binary\n",
    "## classification, while we are in regression (use mse or mae)\n",
    "model_nn.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "initial_train_preds = model_nn.predict([train.user_id, train.item_id])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"weights_nn.hdf5\", verbose=1, save_best_only=True)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "history = model_nn.fit([train.user_id, train.item_id], train.rating,\n",
    "                    batch_size=64, epochs=300, validation_split=0.1,\n",
    "                    shuffle=True, verbose=2, callbacks=[checkpointer, lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error 0.928\n",
      "Root Mean Square Error (rounded) 0.976\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "model_nn.load_weights(\"weights_nn.hdf5\")\n",
    "\n",
    "test_preds = model_nn.predict([test.user_id, test.item_id])\n",
    "print(\"Root Mean Square Error %0.3f\" % np.sqrt(mean_squared_error(test_preds, test.rating)))\n",
    "print(\"Root Mean Square Error (rounded) %0.3f\" % np.sqrt(mean_squared_error(np.round(test_preds), test.rating)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainBest = usersBestMovies(train, 1)\n",
    "testBest = usersBestMovies(test, 1)\n",
    "predBest_cossim = recommendMovies(1, train_data=train, similarity=similarity, prediction_matrix=pred_Matrix)\n",
    "predBest_factor = recommendMovies(1, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When Harry Met Sally... (1989)',\n",
       " 'Lone Star (1996)',\n",
       " 'Welcome to the Dollhouse (1995)',\n",
       " 'French Twist (Gazon maudit) (1995)',\n",
       " 'Haunted World of Edward D. Wood Jr., The (1995)',\n",
       " 'Mystery Science Theater 3000: The Movie (1996)',\n",
       " 'Wrong Trousers, The (1993)',\n",
       " 'Bound (1996)',\n",
       " 'Jean de Florette (1986)',\n",
       " 'Mars Attacks! (1996)',\n",
       " 'Good, The Bad and The Ugly, The (1966)',\n",
       " 'Professional, The (1994)',\n",
       " \"Monty Python's Life of Brian (1979)\",\n",
       " 'Crumb (1994)',\n",
       " '12 Angry Men (1957)',\n",
       " 'Princess Bride, The (1987)',\n",
       " 'Star Wars (1977)',\n",
       " 'Wallace & Gromit: The Best of Aardman Animation (1996)',\n",
       " 'Henry V (1989)',\n",
       " 'Chasing Amy (1997)',\n",
       " 'Graduate, The (1967)',\n",
       " 'Dolores Claiborne (1994)',\n",
       " 'Ridicule (1996)',\n",
       " 'Three Colors: Blue (1993)',\n",
       " 'Priest (1994)',\n",
       " 'Big Night (1996)',\n",
       " 'Eat Drink Man Woman (1994)',\n",
       " 'Return of the Jedi (1983)',\n",
       " 'Three Colors: Red (1994)',\n",
       " 'Chasing Amy (1997)',\n",
       " 'Clerks (1994)',\n",
       " 'Empire Strikes Back, The (1980)',\n",
       " 'Nikita (La Femme Nikita) (1990)',\n",
       " 'Pillow Book, The (1995)',\n",
       " 'Dead Man Walking (1995)',\n",
       " 'Monty Python and the Holy Grail (1974)',\n",
       " 'Nightmare Before Christmas, The (1993)',\n",
       " 'Gattaca (1997)',\n",
       " 'Contact (1997)',\n",
       " 'Aliens (1986)',\n",
       " 'Brazil (1985)',\n",
       " 'Breaking the Waves (1996)',\n",
       " 'Kolya (1996)',\n",
       " \"Antonia's Line (1995)\",\n",
       " 'Kids in the Hall: Brain Candy (1996)',\n",
       " 'Terminator 2: Judgment Day (1991)',\n",
       " \"Mr. Holland's Opus (1995)\",\n",
       " 'Jurassic Park (1993)',\n",
       " 'Swingers (1996)',\n",
       " 'Alien (1979)',\n",
       " 'Manon of the Spring (Manon des sources) (1986)',\n",
       " 'Godfather, The (1972)',\n",
       " 'Sleeper (1973)',\n",
       " 'Star Trek: The Wrath of Khan (1982)',\n",
       " 'Terminator, The (1984)',\n",
       " 'Groundhog Day (1993)',\n",
       " 'Cyrano de Bergerac (1990)',\n",
       " 'Horseman on the Roof, The (Hussard sur le toit, Le) (1995)',\n",
       " 'Hoop Dreams (1994)',\n",
       " 'Back to the Future (1985)',\n",
       " 'Mighty Aphrodite (1995)',\n",
       " 'Hudsucker Proxy, The (1994)',\n",
       " 'Dead Poets Society (1989)',\n",
       " 'Toy Story (1995)',\n",
       " 'Young Frankenstein (1974)']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Searching for Bobby Fischer (1993)',\n",
       " 'Truth About Cats & Dogs, The (1996)',\n",
       " 'Shawshank Redemption, The (1994)',\n",
       " 'Amadeus (1984)',\n",
       " 'Blade Runner (1982)',\n",
       " 'Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)',\n",
       " 'Postino, Il (1994)',\n",
       " 'Maya Lin: A Strong Clear Vision (1994)',\n",
       " 'Delicatessen (1991)',\n",
       " 'Raiders of the Lost Ark (1981)',\n",
       " 'Remains of the Day, The (1993)',\n",
       " 'Fargo (1996)',\n",
       " 'Full Monty, The (1997)',\n",
       " 'Cinema Paradiso (1988)',\n",
       " 'Usual Suspects, The (1995)',\n",
       " 'Sling Blade (1996)']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cyclo (1995)',\n",
       " 'Little City (1998)',\n",
       " 'Coldblooded (1995)',\n",
       " 'Mamma Roma (1962)',\n",
       " 'King of New York (1990)',\n",
       " 'Office Killer (1997)',\n",
       " 'Substance of Fire, The (1996)',\n",
       " 'Ballad of Narayama, The (Narayama Bushiko) (1958)',\n",
       " \"C'est arriv prs de chez vous (1992)\",\n",
       " \"My Life and Times With Antonin Artaud (En compagnie d'Antonin Artaud) (1993)\"]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predBest_cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two or Three Things I Know About Her (1966)',\n",
       " 'In the Line of Duty 2 (1987)',\n",
       " 'Great Day in Harlem, A (1994)',\n",
       " 'Maya Lin: A Strong Clear Vision (1994)',\n",
       " 'Wallace & Gromit: The Best of Aardman Animation (1996)',\n",
       " 'Love and Death on Long Island (1997)',\n",
       " 'Godfather, The (1972)',\n",
       " 'High Noon (1952)',\n",
       " 'Godfather: Part II, The (1974)',\n",
       " 'Laura (1944)']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predBest_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend movies by using different systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommendMovies(userID=1, train_data=train, similarity=similarity,\n",
    "                prediction_matrix=pred_Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
